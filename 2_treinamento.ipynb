{
 "cells": [
  {
   "cell_type": "code",
   "id": "2795701d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:17:39.666066Z",
     "start_time": "2025-02-10T00:17:37.825271Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7da1b3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:17:41.058733Z",
     "start_time": "2025-02-10T00:17:39.666066Z"
    }
   },
   "source": "from ultralytics import YOLO",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Diversas vezes nas células, os modelos serão referidos como 'modelo funcional' e 'experimentos':\n",
    "\n",
    "- Modelo Funcional = modelo que teve o melhor desempenho na detecção de objetos cortantes do vídeo disponibilizado, escolhido para avaliação.\n",
    "- Experimentos = Diversos treinamentos experimentais feitos, combinando vários hiperparâmetros e versões distintas do YOLO. São os que não tiveram um desempenho\n",
    "                 melhor em comparação ao modelo funcional, mas que tiveram ou não um melhor desempenho nas métricas e estatísticas do ultralytics e até mesmo\n",
    "                 nas imagens de 'test' quando postas sob avaliação.\n",
    "'''"
   ],
   "id": "77d9327e6b5abc95"
  },
  {
   "cell_type": "code",
   "id": "fd2b8c92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:28:32.386604Z",
     "start_time": "2025-02-09T23:28:32.298854Z"
    }
   },
   "source": [
    "class ModeloYolo:\n",
    "    def __init__(self, modelo_path, output_dir):\n",
    "        self.modelo = YOLO(modelo_path)\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def treinar(self, dataset_path, epochs_treinamento, imgsz, batch_size, lr0, lrf, augment, verbose, resume, device):\n",
    "        print(\"Iniciando o treinamento...\")\n",
    "\n",
    "        self.modelo.train(\n",
    "            data=dataset_path,\n",
    "            epochs=epochs_treinamento, #50\n",
    "            imgsz=imgsz, #640\n",
    "            batch=batch_size, #32\n",
    "            lr0=lr0, #0.003\n",
    "            lrf=lrf, #0.2\n",
    "            augment=augment, #True\n",
    "            verbose=verbose, #False\n",
    "            resume=resume, #False\n",
    "            device=device #0\n",
    "        )\n",
    "        #Hiperparâmetros extras para otimização de treinamento (não foram usados no treinamento do modelo funcional, apenas em outros experimentos):\n",
    "        '''\n",
    "        workers=8,\n",
    "        pretrained=True,\n",
    "        resume=False,\n",
    "        single_cls=False, \n",
    "        box=7.5,\n",
    "        cls=0.5, \n",
    "        dfl=1.5,\n",
    "        val=True,\n",
    "        degrees=0.3,\n",
    "        hsv_s=0.3,\n",
    "        hsv_v=0.3,\n",
    "        scale=0.5,\n",
    "        fliplr=0.5,\n",
    "        classes=[0],\n",
    "        patience=10\n",
    "        '''\n",
    "    \n",
    "        # Caminho onde o YOLO salva automaticamente o melhor modelo\n",
    "        best_model_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\") #(train1, train2...)\n",
    "        \n",
    "        # Caminho onde salvar o best.pt (melhor modelo do treinamento)\n",
    "        final_model_path = os.path.join(self.output_dir, 'best.pt')\n",
    "        \n",
    "        # Copiar o best.pt para o diretório desejado\n",
    "        if os.path.exists(best_model_path):\n",
    "            shutil.copy(best_model_path, final_model_path)\n",
    "            print(f\"Melhor modelo salvo em: {final_model_path}\")\n",
    "        else:\n",
    "            print(\"Erro: best.pt não encontrado. Verifique o caminho de saída do YOLO.\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f766b25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:37:46.806630Z",
     "start_time": "2025-02-09T23:32:35.957312Z"
    }
   },
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "caminho_modelo = os.path.join(current_dir, 'modelo')  # Diretório onde o modelo treinado será salvo\n",
    "caminho_yaml = os.path.join(current_dir, 'dataset.yaml')  # Caminho para o arquivo YAML de configuração do dataset\n",
    "\n",
    "# Inicializar e treinar o modelo\n",
    "modelo = os.path.join(caminho_modelo,'yolo11s.pt') #yolo11s utilizado tanto no modelo funcional quanto modelo robusto (experimento), o yolo11m não apresentou melhoras significativas\n",
    "\n",
    "classe_modelo = ModeloYolo(modelo, caminho_modelo)\n",
    "\n",
    "'''\n",
    "Considerações antes de executar o treino:\n",
    "    - Device=0 caso possua uma gpu, do contrário, adote 'cpu'\n",
    "    - O treinamento do modelo funcional foi feito com uma placa de vídeoo RTX 3060 TI, tomando 4h de tempo de treinamento com os parâmetros abaixo,\n",
    "      portanto, caso queira apenas testar o fluxo de funcionamento deste projeto, informe de 1 a 5 epochs para não tomar muito tempo. Porém, isso\n",
    "      fará com que o treinamento não desempenhe o mesmo resultado obtido com o modelo funcional.\n",
    "'''\n",
    "\n",
    "classe_modelo.treinar(caminho_yaml, 50, 640, 32, 0.003, 0.2, True, False, False, 0)\n",
    "\n",
    "'''\n",
    "OBS: Durante o treinamento, o YOLO baixa o modelo yolo11n.pt para verificar a compatibilidade do seu ambiente com AMP. Essa verificação assegura que o treinamento realizado de forma eficiente, aproveitando os benefícios da precisão mista. Essa verificação não afeta o modelo que está sendo treinado. O treinamento ocorrerá com o modelo especificado 'yolo11s.pt', e o arquivo yolo11n.pt é utilizado apenas para a verificação de AMP (O AMP reduz o consumo de memória e acelera o treinamento de redes neurais).\n",
    "'''"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\modelo\\yolo11s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18.4M/18.4M [00:03<00:00, 5.60MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o treinamento...\n",
      "New https://pypi.org/project/ultralytics/8.3.74 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.69  Python-3.12.3 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\modelo\\yolo11s.pt, data=C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\dataset.yaml, epochs=3, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.003, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    820182  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "YOLO11s summary: 319 layers, 9,428,566 parameters, 9,428,550 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:01<00:00, 4.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\arquivo\\imagens\\dataset-hackathon\\train\\labels... 4409 images, 20 backgrounds, 0 corrupt: 100%|██████████| 4409/4409 [00:04<00:00, 1073.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\arquivo\\imagens\\dataset-hackathon\\train\\labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 3, len(boxes) = 5110. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\arquivo\\imagens\\dataset-hackathon\\valid\\labels... 1043 images, 7 backgrounds, 0 corrupt: 100%|██████████| 1043/1043 [00:01<00:00, 762.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\arquivo\\imagens\\dataset-hackathon\\valid\\labels.cache\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.003' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train\u001B[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3       2.4G      1.485      2.533      1.717          1        640: 100%|██████████| 552/552 [01:04<00:00,  8.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 66/66 [00:08<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1043       1203      0.259      0.223      0.193     0.0946\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      2.38G      1.549      2.153      1.804          1        640: 100%|██████████| 552/552 [00:58<00:00,  9.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 66/66 [00:08<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1043       1203      0.395       0.31      0.277      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      2.36G      1.401      1.916      1.701          2        640: 100%|██████████| 552/552 [00:55<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 66/66 [00:06<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1043       1203       0.57      0.498      0.527      0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.058 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 19.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 19.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.69  Python-3.12.3 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "YOLO11s summary (fused): 238 layers, 9,413,574 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 66/66 [00:11<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1043       1203      0.688      0.499       0.59      0.391\n",
      "Speed: 0.3ms preprocess, 7.4ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\train\u001B[0m\n",
      "Melhor modelo salvo em: C:\\Users\\Pichau\\PycharmProjects\\HACKATHON-MAIN\\Hackathon\\modelo\\best.pt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:31:31.523082Z",
     "start_time": "2025-02-10T00:31:18.258545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "caminho_imagens = os.path.join(current_dir, 'arquivo', 'imagens', 'dataset-hackathon', 'test', 'images')  # Pasta onde as imagens de teste estão\n",
    "caminho_modelo = os.path.join(current_dir, 'modelo')  # Diretório do modelo\n",
    "caminho_modelo_treinado = os.path.join(caminho_modelo,'best.pt')  # Diretório do modelo treinado\n",
    "\n",
    "modelo = YOLO(caminho_modelo_treinado)\n",
    "'''\n",
    "Para testar com o modelo funcional:\n",
    "modelo = YOLO('modelo-funcional/weights/best.pt')\n",
    "'''\n",
    "\n",
    "# Seleciona as 5 primeiras imagens da pasta de teste\n",
    "imagens = sorted(glob.glob(caminho_imagens+'/*.jpg'))[:5]\n",
    "\n",
    "modelo.predict(\n",
    "    source=imagens,     # Arquivos, pasta ou URL\n",
    "    imgsz=640,          # Tamanho da imagem (padrão: 640)\n",
    "    conf=0.5,           # Confiança mínima para detectar um objeto\n",
    "    iou=0.5,            # IOU mínimo para supressão de NMS (padrão: 0.7)\n",
    "    device=0,           # Define a GPU (0) ou CPU ('cpu')\n",
    "    save=True,          # Salvar imagens com predições\n",
    "    save_txt=True,      # Salvar resultados em arquivos .txt\n",
    "    save_conf=True,     # Incluir valores de confiança nos .txt salvos\n",
    "    show=False,          # Exibir predições na tela\n",
    "    line_thickness=2,   # Espessura das caixas delimitadoras\n",
    "    hide_labels=False,  # Esconder os rótulos dos objetos detectados\n",
    "    hide_conf=False,    # Esconder valores de confiança das caixas\n",
    "    classes=[0],        # Filtrar apenas certas classes (0 = knife, 1 = pistol)\n",
    "    half=True,          # Usa precisão mista (reduz consumo de memória na GPU)\n",
    "    augment=False,      # Usa técnicas de aumento de dados na inferência\n",
    "    agnostic_nms=True,  # Aplica NMS sem considerar classes diferentes\n",
    "    visualize=True,     # Gera mapas de ativação para melhor interpretação\n",
    "    project=\"resultado-test\",   # Define a pasta de saída\n",
    "    name=\"teste-modelo\"         # Nome da subpasta dentro de 'project'\n",
    ")"
   ],
   "id": "34868acea62a64b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  'line_thickness' is deprecated and will be removed in in the future. Use 'line_width' instead.\n",
      "WARNING  'hide_labels' is deprecated and will be removed in in the future. Use 'show_labels' instead.\n",
      "WARNING  'hide_conf' is deprecated and will be removed in in the future. Use 'show_conf' instead.\n",
      "\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage0_Conv_features.png... (32/32)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage1_Conv_features.png... (32/64)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage2_C3k2_features.png... (32/128)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage3_Conv_features.png... (32/128)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage4_C3k2_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage5_Conv_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage6_C3k2_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage7_Conv_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage8_C3k2_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage9_SPPF_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage10_C2PSA_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage11_Upsample_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage12_Concat_features.png... (32/768)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage13_C3k2_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage14_Upsample_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage15_Concat_features.png... (32/512)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage16_C3k2_features.png... (32/128)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage17_Conv_features.png... (32/128)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage18_Concat_features.png... (32/384)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage19_C3k2_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage20_Conv_features.png... (32/256)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage21_Concat_features.png... (32/768)\n",
      "Saving resultado-test\\teste-modelo2\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9\\stage22_C3k2_features.png... (32/512)\n",
      "0: 640x640 5 knifes, 2534.8ms\n",
      "1: 640x640 1 knife, 2534.8ms\n",
      "2: 640x640 1 knife, 2534.8ms\n",
      "3: 640x640 1 knife, 2534.8ms\n",
      "4: 640x640 1 knife, 2534.8ms\n",
      "Speed: 2.3ms preprocess, 2534.8ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001B[1mresultado-test\\teste-modelo2\u001B[0m\n",
      "5 labels saved to resultado-test\\teste-modelo2\\labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'knife', 1: 'pistol'}\n",
       " obb: None\n",
       " orig_img: array([[[ 20,  17,  13],\n",
       "         [ 21,  18,  14],\n",
       "         [ 21,  18,  14],\n",
       "         ...,\n",
       "         [119, 162, 205],\n",
       "         [120, 162, 207],\n",
       "         [119, 164, 208]],\n",
       " \n",
       "        [[ 18,  15,  11],\n",
       "         [ 19,  16,  12],\n",
       "         [ 20,  17,  13],\n",
       "         ...,\n",
       "         [135, 177, 220],\n",
       "         [133, 175, 220],\n",
       "         [130, 175, 219]],\n",
       " \n",
       "        [[ 15,  12,   8],\n",
       "         [ 17,  14,  10],\n",
       "         [ 19,  16,  12],\n",
       "         ...,\n",
       "         [136, 178, 220],\n",
       "         [135, 178, 221],\n",
       "         [134, 177, 220]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[159, 197, 232],\n",
       "         [158, 196, 231],\n",
       "         [157, 195, 230],\n",
       "         ...,\n",
       "         [179, 182, 190],\n",
       "         [148, 150, 161],\n",
       "         [131, 135, 146]],\n",
       " \n",
       "        [[159, 197, 232],\n",
       "         [158, 196, 231],\n",
       "         [157, 195, 230],\n",
       "         ...,\n",
       "         [223, 226, 234],\n",
       "         [215, 217, 227],\n",
       "         [204, 206, 216]],\n",
       " \n",
       "        [[159, 197, 232],\n",
       "         [158, 196, 231],\n",
       "         [157, 195, 230],\n",
       "         ...,\n",
       "         [206, 209, 214],\n",
       "         [207, 209, 219],\n",
       "         [192, 194, 204]]], dtype=uint8)\n",
       " orig_shape: (300, 300)\n",
       " path: 'C:\\\\Users\\\\Pichau\\\\PycharmProjects\\\\HACKATHON-MAIN\\\\Hackathon\\\\arquivo\\\\imagens\\\\dataset-hackathon\\\\test\\\\images\\\\--------_------_jpg.rf.4eb0868f6cd41827c921043ddfa37ff9.jpg'\n",
       " probs: None\n",
       " save_dir: 'resultado-test\\\\teste-modelo2'\n",
       " speed: {'preprocess': 2.3012638092041016, 'inference': 2534.8432064056396, 'postprocess': 3.3782005310058594},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'knife', 1: 'pistol'}\n",
       " obb: None\n",
       " orig_img: array([[[194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         ...,\n",
       "         [213, 207, 166],\n",
       "         [213, 207, 166],\n",
       "         [214, 208, 167]],\n",
       " \n",
       "        [[194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         ...,\n",
       "         [213, 207, 166],\n",
       "         [213, 207, 166],\n",
       "         [214, 208, 167]],\n",
       " \n",
       "        [[194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         [194, 185, 148],\n",
       "         ...,\n",
       "         [213, 207, 166],\n",
       "         [213, 207, 166],\n",
       "         [214, 208, 167]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 14,   7, 104],\n",
       "         [ 15,   7, 107],\n",
       "         [ 16,   9, 113],\n",
       "         ...,\n",
       "         [  1,  10,  37],\n",
       "         [  3,   9,  38],\n",
       "         [  4,   9,  40]],\n",
       " \n",
       "        [[ 16,   8, 103],\n",
       "         [ 15,   8, 105],\n",
       "         [ 16,   9, 113],\n",
       "         ...,\n",
       "         [  1,  10,  37],\n",
       "         [  3,   9,  38],\n",
       "         [  4,   9,  40]],\n",
       " \n",
       "        [[ 16,   8, 103],\n",
       "         [ 15,   8, 105],\n",
       "         [ 18,   9, 113],\n",
       "         ...,\n",
       "         [  1,  10,  37],\n",
       "         [  3,   9,  38],\n",
       "         [  4,   9,  40]]], dtype=uint8)\n",
       " orig_shape: (300, 300)\n",
       " path: 'C:\\\\Users\\\\Pichau\\\\PycharmProjects\\\\HACKATHON-MAIN\\\\Hackathon\\\\arquivo\\\\imagens\\\\dataset-hackathon\\\\test\\\\images\\\\--------_------_jpg.rf.73a95f4c2fe5cfa9c071b8f4d2a02139.jpg'\n",
       " probs: None\n",
       " save_dir: 'resultado-test\\\\teste-modelo2'\n",
       " speed: {'preprocess': 2.3012638092041016, 'inference': 2534.8432064056396, 'postprocess': 3.3782005310058594},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'knife', 1: 'pistol'}\n",
       " obb: None\n",
       " orig_img: array([[[209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         ...,\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167]],\n",
       " \n",
       "        [[209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         ...,\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167]],\n",
       " \n",
       "        [[209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         [209, 194, 155],\n",
       "         ...,\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167],\n",
       "         [227, 215, 167]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 43,  36, 140],\n",
       "         [ 61,  56, 161],\n",
       "         [ 65,  59, 168],\n",
       "         ...,\n",
       "         [ 96, 137, 159],\n",
       "         [ 94, 136, 155],\n",
       "         [ 94, 136, 155]],\n",
       " \n",
       "        [[ 47,  42, 147],\n",
       "         [ 64,  59, 164],\n",
       "         [ 66,  60, 169],\n",
       "         ...,\n",
       "         [ 99, 140, 162],\n",
       "         [ 96, 139, 158],\n",
       "         [ 96, 139, 158]],\n",
       " \n",
       "        [[ 49,  44, 149],\n",
       "         [ 66,  60, 167],\n",
       "         [ 64,  59, 168],\n",
       "         ...,\n",
       "         [102, 143, 165],\n",
       "         [ 99, 142, 161],\n",
       "         [ 98, 141, 160]]], dtype=uint8)\n",
       " orig_shape: (300, 300)\n",
       " path: 'C:\\\\Users\\\\Pichau\\\\PycharmProjects\\\\HACKATHON-MAIN\\\\Hackathon\\\\arquivo\\\\imagens\\\\dataset-hackathon\\\\test\\\\images\\\\--------_------_jpg.rf.a34ba4c1ed5c93d2f9bb8289a50a4e46.jpg'\n",
       " probs: None\n",
       " save_dir: 'resultado-test\\\\teste-modelo2'\n",
       " speed: {'preprocess': 2.3012638092041016, 'inference': 2534.8432064056396, 'postprocess': 3.3782005310058594},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'knife', 1: 'pistol'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [252, 252, 252]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [249, 249, 249],\n",
       "         [254, 254, 254],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[252, 255, 253],\n",
       "         [252, 255, 253],\n",
       "         [252, 255, 253],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (300, 300)\n",
       " path: 'C:\\\\Users\\\\Pichau\\\\PycharmProjects\\\\HACKATHON-MAIN\\\\Hackathon\\\\arquivo\\\\imagens\\\\dataset-hackathon\\\\test\\\\images\\\\445_jpg.rf.2e04379013684f454abbc00564910fcc.jpg'\n",
       " probs: None\n",
       " save_dir: 'resultado-test\\\\teste-modelo2'\n",
       " speed: {'preprocess': 2.3012638092041016, 'inference': 2534.8432064056396, 'postprocess': 3.3782005310058594},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'knife', 1: 'pistol'}\n",
       " obb: None\n",
       " orig_img: array([[[ 27,  25,  24],\n",
       "         [ 26,  24,  23],\n",
       "         [ 24,  22,  21],\n",
       "         ...,\n",
       "         [141, 128, 130],\n",
       "         [141, 128, 130],\n",
       "         [140, 127, 129]],\n",
       " \n",
       "        [[ 26,  24,  23],\n",
       "         [ 26,  24,  23],\n",
       "         [ 24,  22,  21],\n",
       "         ...,\n",
       "         [140, 127, 129],\n",
       "         [140, 127, 129],\n",
       "         [139, 126, 128]],\n",
       " \n",
       "        [[ 26,  24,  23],\n",
       "         [ 25,  23,  22],\n",
       "         [ 25,  23,  22],\n",
       "         ...,\n",
       "         [140, 127, 129],\n",
       "         [139, 126, 128],\n",
       "         [139, 126, 128]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 14,  55,  18],\n",
       "         [ 12,  55,  18],\n",
       "         [ 15,  56,  19],\n",
       "         ...,\n",
       "         [ 47, 108,  40],\n",
       "         [ 47, 108,  40],\n",
       "         [ 47, 108,  40]],\n",
       " \n",
       "        [[ 24, 114,  19],\n",
       "         [ 22, 114,  19],\n",
       "         [ 24, 114,  19],\n",
       "         ...,\n",
       "         [  1, 125,   1],\n",
       "         [  1, 125,   1],\n",
       "         [  1, 125,   1]],\n",
       " \n",
       "        [[ 13, 129,   6],\n",
       "         [ 13, 129,   6],\n",
       "         [ 13, 129,   6],\n",
       "         ...,\n",
       "         [  0, 142,   0],\n",
       "         [  0, 142,   0],\n",
       "         [  0, 142,   0]]], dtype=uint8)\n",
       " orig_shape: (300, 300)\n",
       " path: 'C:\\\\Users\\\\Pichau\\\\PycharmProjects\\\\HACKATHON-MAIN\\\\Hackathon\\\\arquivo\\\\imagens\\\\dataset-hackathon\\\\test\\\\images\\\\ABbframe00277_jpg.rf.1b722bd1914427b42b9579722538945d.jpg'\n",
       " probs: None\n",
       " save_dir: 'resultado-test\\\\teste-modelo2'\n",
       " speed: {'preprocess': 2.3012638092041016, 'inference': 2534.8432064056396, 'postprocess': 3.3782005310058594}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
